{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emotion and engagement prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtdVhYFb7Bj1",
        "colab_type": "text"
      },
      "source": [
        "# **Mini project: Emotion and engagement prediction from Facebook news posts**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PKZ9j5TBgHn",
        "colab_type": "code",
        "outputId": "ccd2a27f-b00e-442c-8579-c4d7e866c1af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(tf.__version__)\n",
        "print(\"EE enabled?\", tf.executing_eagerly())\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import itertools\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import pickle\n",
        "\n",
        "def convert_to_pickle(item, directory):\n",
        "    pickle.dump(item, open(directory,\"wb\"))\n",
        "\n",
        "def load_from_pickle(directory):\n",
        "    return pickle.load(open(directory,\"rb\"))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc2\n",
            "EE enabled? True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRicc_1s7U-K",
        "colab_type": "text"
      },
      "source": [
        "# 1) Load news datasets from Google Drive:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WX08XI8hCDFq",
        "colab_type": "code",
        "outputId": "47476f89-9502-414d-9ad2-00d97836a057",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPwBK_Ti8FeC",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "1.   Concatenate news datasets\n",
        "2.   Drop rows with missing data in dataframe\n",
        "3.   Drop rows with no reactions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mES5LUtlCTj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bson \n",
        "import pandas as pd\n",
        "\n",
        "trump_data = pd.read_csv(\"/gdrive/My Drive/trump_filtered_data.csv\") \n",
        "\n",
        "fox_news = pd.read_csv(\"/gdrive/My Drive/news_datasets/fox_news_posts.csv\") \n",
        "cnn_news = pd.read_csv(\"/gdrive/My Drive/news_datasets/CNN_news_posts.csv\") \n",
        "bbc_news = pd.read_csv(\"/gdrive/My Drive/news_datasets/bbc_news.csv\")\n",
        "nbc_news = pd.read_csv(\"/gdrive/My Drive/news_datasets/nbc_news.csv\") \n",
        "npr_news = pd.read_csv(\"/gdrive/My Drive/news_datasets/npr_news.csv\") \n",
        "the_los_angeles_times_news = pd.read_csv(\"/gdrive/My Drive/news_datasets/the_los_angeles_times_news.csv\") \n",
        "cbs_news = pd.read_csv(\"/gdrive/My Drive/news_datasets/cbs_news.csv\") \n",
        "fox_news2 = pd.read_csv(\"/gdrive/My Drive/news_datasets/fox_news2.csv\") \n",
        "abc_news = pd.read_csv(\"/gdrive/My Drive/news_datasets/abc_news.csv\") \n",
        "\n",
        "\n",
        "DF_list = list()\n",
        "DF_list.append(fox_news)\n",
        "DF_list.append(cnn_news)\n",
        "DF_list.append(bbc_news)\n",
        "DF_list.append(nbc_news)\n",
        "DF_list.append(npr_news)\n",
        "DF_list.append(the_los_angeles_times_news)\n",
        "DF_list.append(cbs_news)\n",
        "DF_list.append(fox_news2)\n",
        "DF_list.append(abc_news)\n",
        "\n",
        "process_list = list()\n",
        "\n",
        "for it in DF_list:\n",
        "  item = it[['message', 'post_type', 'likes_count', 'comments_count', 'shares_count', 'love_count', 'wow_count', 'haha_count', 'sad_count', 'angry_count']]\n",
        "  process_list.append(item)\n",
        "\n",
        "d = pd.concat(process_list, axis=0, sort=True)\n",
        "da = d[['message', 'post_type', 'likes_count', 'comments_count', 'shares_count', 'love_count', 'wow_count', 'haha_count', 'sad_count', 'angry_count']]\n",
        "dat = da.rename(columns={\"message\":\"text\", \"likes_count\":\"likes\", \"comments_count\":\"comments\", \"shares_count\":\"shares\", \"love_count\":\"loves\", \"wow_count\":\"wows\", \"haha_count\":\"hahas\", \"sad_count\":\"sads\", \"angry_count\":\"angrys\"})\n",
        "dat = dat.dropna()\n",
        "dat = dat[(dat['loves'] != 0) | (dat['wows'] != 0) | (dat['hahas'] != 0) | (dat['sads'] != 0) | (dat['angrys'] != 0)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hipISRQK-CY-",
        "colab_type": "text"
      },
      "source": [
        "# 2) Preprocess target data: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6gH1Bcg0onX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "f52f26b0-2249-4df2-860d-136e620cd779"
      },
      "source": [
        "def change_news_labels(data):\n",
        "  dataset = data.copy()\n",
        "  likes = dataset['likes']\n",
        "  comments = dataset['comments']\n",
        "  shares = dataset['shares']\n",
        "\n",
        "  loves = np.array(dataset['loves']) \n",
        "  wows = np.array(dataset['wows']) \n",
        "  hahas = np.array(dataset['hahas']) \n",
        "  sads = np.array(dataset['sads']) \n",
        "  angrys = np.array(dataset['angrys']) \n",
        "\n",
        "  loves_labels = []\n",
        "  wows_labels = []\n",
        "  hahas_labels = []\n",
        "  sads_labels = []\n",
        "  angrys_labels = []\n",
        "\n",
        "  for i in range(len(np.array(dat))):\n",
        "    lo = loves[i]\n",
        "    wo = wows[i]\n",
        "    ha = hahas[i]\n",
        "    sa = sads[i]\n",
        "    ang = angrys[i]\n",
        "\n",
        "    emo_list = [lo, wo, ha, sa, ang]\n",
        "    sum_count = np.sum(emo_list)\n",
        "    love = (lo/sum_count)\n",
        "    wow = (wo/sum_count)\n",
        "    haha = (ha/sum_count)\n",
        "    sad = (sa/sum_count)\n",
        "    angry = (ang/sum_count)\n",
        "\n",
        "    loves_labels.append(love)\n",
        "    wows_labels.append(wow)\n",
        "    hahas_labels.append(haha)\n",
        "    sads_labels.append(sad)\n",
        "    angrys_labels.append(angry)\n",
        "\n",
        "  \n",
        "  likes_med1 = np.percentile(likes, 33)\n",
        "  likes_med2 = np.percentile(likes, 66)\n",
        "  comments_med1 = np.percentile(comments, 33)\n",
        "  comments_med2 = np.percentile(comments, 66)\n",
        "  shares_med1 = np.percentile(shares, 33)\n",
        "  shares_med2 = np.percentile(shares, 66)\n",
        "\n",
        "  likes = [\"Low\" if l < likes_med1 else \"Medium\" if l < likes_med2 else \"High\" for l in likes]\n",
        "  comments = [\"Low\" if l < comments_med1 else \"Medium\" if l < comments_med2 else \"High\" for l in comments]\n",
        "  shares = [\"Low\" if l < shares_med1 else \"Medium\" if l < shares_med2 else \"High\" for l in shares]\n",
        "\n",
        "  dataset['likes'] = likes\n",
        "  dataset['comments'] = comments\n",
        "  dataset['shares'] = shares\n",
        "  dataset['loves'] =  loves_labels\n",
        "  dataset['wows'] = wows_labels\n",
        "  dataset['hahas'] = hahas_labels\n",
        "  dataset['sads'] =  sads_labels\n",
        "  dataset['angrys'] = angrys_labels\n",
        "  \n",
        "  return dataset\n",
        "\n",
        "data = change_news_labels(dat)\n",
        "data.drop_duplicates(subset =\"text\", keep = 'first', inplace = True) \n",
        "data"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>post_type</th>\n",
              "      <th>likes</th>\n",
              "      <th>comments</th>\n",
              "      <th>shares</th>\n",
              "      <th>loves</th>\n",
              "      <th>wows</th>\n",
              "      <th>hahas</th>\n",
              "      <th>sads</th>\n",
              "      <th>angrys</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>That's ten twins in all!</td>\n",
              "      <td>link</td>\n",
              "      <td>High</td>\n",
              "      <td>Medium</td>\n",
              "      <td>High</td>\n",
              "      <td>0.343109</td>\n",
              "      <td>0.607038</td>\n",
              "      <td>0.035191</td>\n",
              "      <td>0.005865</td>\n",
              "      <td>0.008798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BREAKING NEWS: Fox News projects Donald J. Tru...</td>\n",
              "      <td>photo</td>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "      <td>0.392157</td>\n",
              "      <td>0.078431</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.098039</td>\n",
              "      <td>0.372549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>'A couple of months ago we weren't expected to...</td>\n",
              "      <td>video</td>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "      <td>0.400749</td>\n",
              "      <td>0.048689</td>\n",
              "      <td>0.064919</td>\n",
              "      <td>0.218477</td>\n",
              "      <td>0.267166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Do you agree with Jay Sekulow?</td>\n",
              "      <td>link</td>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "      <td>0.133621</td>\n",
              "      <td>0.019397</td>\n",
              "      <td>0.021552</td>\n",
              "      <td>0.112069</td>\n",
              "      <td>0.713362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bill Gates said a court order requiring Apple ...</td>\n",
              "      <td>link</td>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "      <td>0.309589</td>\n",
              "      <td>0.063014</td>\n",
              "      <td>0.038356</td>\n",
              "      <td>0.032877</td>\n",
              "      <td>0.556164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14780</th>\n",
              "      <td>High turnout among Latino voters -- including ...</td>\n",
              "      <td>link</td>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "      <td>Medium</td>\n",
              "      <td>0.686976</td>\n",
              "      <td>0.019005</td>\n",
              "      <td>0.048072</td>\n",
              "      <td>0.013415</td>\n",
              "      <td>0.232532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14781</th>\n",
              "      <td>This New Jersey family went to extraordinary m...</td>\n",
              "      <td>video</td>\n",
              "      <td>Medium</td>\n",
              "      <td>High</td>\n",
              "      <td>Medium</td>\n",
              "      <td>0.197331</td>\n",
              "      <td>0.062917</td>\n",
              "      <td>0.684461</td>\n",
              "      <td>0.011439</td>\n",
              "      <td>0.043851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14783</th>\n",
              "      <td>As Election Day arrives, one of the campaign's...</td>\n",
              "      <td>link</td>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "      <td>0.021544</td>\n",
              "      <td>0.057765</td>\n",
              "      <td>0.291667</td>\n",
              "      <td>0.015388</td>\n",
              "      <td>0.613636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14784</th>\n",
              "      <td>Donald J. Trump's path to victory isn't clear ...</td>\n",
              "      <td>link</td>\n",
              "      <td>Medium</td>\n",
              "      <td>High</td>\n",
              "      <td>Low</td>\n",
              "      <td>0.414179</td>\n",
              "      <td>0.010261</td>\n",
              "      <td>0.150187</td>\n",
              "      <td>0.020522</td>\n",
              "      <td>0.404851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14785</th>\n",
              "      <td>Hillary Clinton is hitting three important sta...</td>\n",
              "      <td>link</td>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "      <td>Medium</td>\n",
              "      <td>0.552671</td>\n",
              "      <td>0.011128</td>\n",
              "      <td>0.045994</td>\n",
              "      <td>0.016320</td>\n",
              "      <td>0.373887</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>76348 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  ...    angrys\n",
              "0                               That's ten twins in all!  ...  0.008798\n",
              "1      BREAKING NEWS: Fox News projects Donald J. Tru...  ...  0.372549\n",
              "2      'A couple of months ago we weren't expected to...  ...  0.267166\n",
              "3                         Do you agree with Jay Sekulow?  ...  0.713362\n",
              "4      Bill Gates said a court order requiring Apple ...  ...  0.556164\n",
              "...                                                  ...  ...       ...\n",
              "14780  High turnout among Latino voters -- including ...  ...  0.232532\n",
              "14781  This New Jersey family went to extraordinary m...  ...  0.043851\n",
              "14783  As Election Day arrives, one of the campaign's...  ...  0.613636\n",
              "14784  Donald J. Trump's path to victory isn't clear ...  ...  0.404851\n",
              "14785  Hillary Clinton is hitting three important sta...  ...  0.373887\n",
              "\n",
              "[76348 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PL6pSS01-Vlp",
        "colab_type": "text"
      },
      "source": [
        "**Heatmap of emotion correlations from Spearman matrix:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxzNUxvr9SjU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "c612a488-8322-48b8-a99d-391e0209cdeb"
      },
      "source": [
        "import numpy as np; np.random.seed(0)\n",
        "import seaborn as sns; sns.set()\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "def get_spearman_correlation_heatmap(em_data):\n",
        "  res = spearmanr(em_data)\n",
        "  print(\"Spearman correlation matrix: \")\n",
        "  print(res[0])\n",
        "  ax = sns.heatmap(res[0], yticklabels=em_data.columns, xticklabels=em_data.columns, vmin=-1, vmax=1, cmap='RdBu_r')\n",
        "  return ax\n",
        "\n",
        "heatmap = get_spearman_correlation_heatmap(data.iloc[:, 5:10])\n",
        "print(\" \")\n",
        "print(\"HEATMAP:\")\n",
        "heatmap"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spearman correlation matrix: \n",
            "[[ 1.         -0.20935629  0.01915831 -0.55275802 -0.42213044]\n",
            " [-0.20935629  1.         -0.03329411 -0.07624948 -0.17362041]\n",
            " [ 0.01915831 -0.03329411  1.         -0.37301714  0.1607878 ]\n",
            " [-0.55275802 -0.07624948 -0.37301714  1.          0.21732631]\n",
            " [-0.42213044 -0.17362041  0.1607878   0.21732631  1.        ]]\n",
            " \n",
            "HEATMAP:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f2c0b6fbc50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1-JmYB4LN1E",
        "colab_type": "text"
      },
      "source": [
        "# 3) Preprocessing textual data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzIEcsAJ-6Gy",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "1.   Remove URLs\n",
        "2.   Remove numbers\n",
        "3.   Remove punctuation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18ITZaPRS_7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def remove_urls(vTEXT):\n",
        "    vTEXT = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', vTEXT, flags=re.MULTILINE)\n",
        "    return(vTEXT)\n",
        "\n",
        "def preprocess_data(sentence):\n",
        "  URLless_string = remove_urls(sentence)\n",
        "  no_number = re.sub(r'\\d+', '', URLless_string)\n",
        "  exclude = set(string.punctuation)\n",
        "  result = ''.join(ch for ch in no_number if ch not in exclude)  \n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaKZcAzp_hb6",
        "colab_type": "text"
      },
      "source": [
        "**Construct the Vocabulary and Index-Word mapping:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W83UOk3ZEOoc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# retain only text that contain less that 70 tokens to avoid too much padding\n",
        "data[\"token_size\"] = data[\"text\"].apply(lambda x: len(x.split(' ')))\n",
        "data = data.loc[data['token_size'] < 70].copy()\n",
        "\n",
        "# This class creates a word-index mapping (e.g,. \"house\" -> 6) and vice-versa (e.g., 6 -> \"house\") for the dataset\n",
        "class ConstructVocab():\n",
        "    def __init__(self, sentences):\n",
        "        self.sentences = sentences\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.vocab = set()\n",
        "        self.create_index()\n",
        "        \n",
        "    def create_index(self):\n",
        "        for s in self.sentences:\n",
        "            # update with individual tokens\n",
        "            self.vocab.update(s.split(' '))\n",
        "            \n",
        "        # sort the vocab\n",
        "        self.vocab = sorted(self.vocab)\n",
        "\n",
        "        # add a padding token with index 0\n",
        "        self.word2idx['<pad>'] = 0\n",
        "        \n",
        "        # word to index mapping\n",
        "        for index, word in enumerate(self.vocab):\n",
        "            self.word2idx[word] = index + 1 # +1 because of pad token\n",
        "        \n",
        "        # index to word mapping\n",
        "        for word, index in self.word2idx.items():\n",
        "            self.idx2word[index] = word  \n",
        "\n",
        "\n",
        "x = [preprocess_data(s) for s in data[\"text\"].values]\n",
        "inputs = ConstructVocab(x)\n",
        "input_tensor = [[inputs.word2idx[s] for s in es.split(' ')]  for es in x]\n",
        "\n",
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "max_length_inp = max_length(input_tensor)\n",
        "input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
        "                                                             maxlen=max_length_inp,\n",
        "                                                             padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSRyTb5TAKO3",
        "colab_type": "text"
      },
      "source": [
        "**Convert target data to suitable format for training:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c-VmkoDMNT2R",
        "colab": {}
      },
      "source": [
        "def encode_labels(arr):\n",
        "  a = [[0,0,1] if e==\"High\" else [0,1,0] if e==\"Medium\" else [1,0,0] for e in arr]\n",
        "  return a \n",
        "\n",
        "\n",
        "likes = np.array(data['likes'])\n",
        "comments = np.array(data['comments'])\n",
        "shares = np.array(data['shares'])\n",
        "loves = np.array(data['loves'])\n",
        "wows = np.array(data['wows'])\n",
        "hahas = np.array(data['hahas'])\n",
        "sads = np.array(data['sads'])\n",
        "angrys = np.array(data['angrys'])\n",
        "\n",
        "post_types = np.array(data[\"post_type\"])\n",
        "\n",
        "likes_target = encode_labels(likes.reshape(len(likes), 1))\n",
        "comments_target = encode_labels(comments.reshape(len(comments), 1))\n",
        "shares_target = encode_labels(shares.reshape(len(shares), 1))\n",
        "loves_target = loves.reshape(len(loves), 1)\n",
        "wows_target = wows.reshape(len(wows), 1)\n",
        "hahas_target = hahas.reshape(len(hahas), 1)\n",
        "sads_target = sads.reshape(len(sads), 1)\n",
        "angrys_target = angrys.reshape(len(angrys), 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWSXtjpDGFRR",
        "colab_type": "text"
      },
      "source": [
        "# 4) Split data\n",
        "\n",
        "We split our data into a train and validation set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0koeqJqwLfb8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_test, post_type_train, post_type_test, like_train, like_test, comment_train, comment_test, share_train, share_test, love_train, love_test, wow_train, wow_test, haha_train, haha_test, sad_train, sad_test, angry_train, angry_test = train_test_split(input_tensor, post_types, likes_target, comments_target, shares_target, loves_target, wows_target, hahas_target, sads_target, angrys_target, test_size=0.3, random_state=42)\n",
        "\n",
        "y_train = np.concatenate((love_train, wow_train, haha_train, sad_train, angry_train), axis=1)\n",
        "y_test = np.concatenate((love_test, wow_test, haha_test, sad_test, angry_test), axis=1)\n",
        "\n",
        "y_eng_train = np.concatenate((comment_train, share_train, like_train), axis=1)\n",
        "y_eng_test = np.concatenate((comment_test, share_test, like_test), axis=1)\n",
        "\n",
        "post_type_tra = np.reshape(post_type_train, (post_type_train.shape[0], 1))\n",
        "post_type_te = np.reshape(post_type_test, (post_type_test.shape[0], 1))\n",
        "posts_train =  np.reshape(post_type_tra, (post_type_tra.shape[0], 1))\n",
        "posts_test =  np.reshape(post_type_te, (post_type_te.shape[0], 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "um_VaOVFXSQj",
        "colab_type": "text"
      },
      "source": [
        "# 5) Emotion prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxPsbxEUDF9O",
        "colab_type": "text"
      },
      "source": [
        "We load the data into a data loader, which makes it easy to manipulate the data, create batches, and apply further transformations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzhSDj7WN3In",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_BUFFER_SIZE = (x_train.shape[0])\n",
        "TEST_BUFFER_SIZE = (x_test.shape[0])\n",
        "BATCH_SIZE = 128\n",
        "TRAIN_N_BATCH = TRAIN_BUFFER_SIZE // BATCH_SIZE\n",
        "TEST_N_BATCH = TEST_BUFFER_SIZE // BATCH_SIZE\n",
        "\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inputs.word2idx)\n",
        "target_size = y_train.shape[1]\n",
        "inp_shape = x_train.shape[1]\n",
        "\n",
        "# emotion datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(TRAIN_BUFFER_SIZE) #add posts_training here for engagement prediction\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).shuffle(TEST_BUFFER_SIZE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qthBcMYKGcUA",
        "colab_type": "text"
      },
      "source": [
        "### Implementing a custom loss function to account for emotions correlations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKUCt15YnU-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "def Custom_KLD_loss(y, prediction): # add posts here\n",
        "\n",
        "    pred = ([[0 if t<=0.2 else 1 for t in preds] for preds in (prediction)])\n",
        "    ground = ([[0 if t<=0.2 else 1 for t in gr] for gr in (y)])\n",
        "    li = []\n",
        "    for i in range(len(pred)):\n",
        "      if ((ground[i][3] == ground[i][4]) and (pred[i][3] != pred[i][4])):\n",
        "        li.append(i)\n",
        "      if ((ground[i][0] != ground[i][3]) and (pred[i][0] == pred[i][3])):\n",
        "        li.append(i)\n",
        "    \n",
        "    mask_array = np.ones((128,), dtype=np.float32)\n",
        "    for index in li: \n",
        "      mask_array[index] = mask_array[index]*2\n",
        "\n",
        "    mask = tf.convert_to_tensor(mask_array)\n",
        "    k = tf.keras.losses.KLD\n",
        "    loss = k(y, prediction)\n",
        "    return loss*mask\n",
        "\n",
        " \n",
        "def custom_accuracy(y, yhat):\n",
        "    r1 = tf.math.greater(y, 0.2)\n",
        "    r2 = tf.math.greater(yhat, 0.2)\n",
        "    match = tf.math.equal(r1, r2)\n",
        "    n_matches = tf.reduce_sum(tf.cast(match, tf.float32))\n",
        "    len_y = tf.cast(tf.size(y), tf.float32)\n",
        "    acc = (n_matches/(len_y))\n",
        "    return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWeIndQIER9R",
        "colab_type": "text"
      },
      "source": [
        "### Constructing the GRU model (as a subclass of keras Model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5PCaHWAzYci",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "e4b4e92a-e8e4-42f6-969e-1f96c65f5e35"
      },
      "source": [
        "### define the GRU component\n",
        "def gru(units):\n",
        "  # Using CuDNNGRU(provides a 3x speedup than GRU) when it is available\n",
        "  # the code automatically does that.\n",
        "    if tf.test.is_gpu_available():\n",
        "        print(\"YES: CuDNNGRU available\")\n",
        "        return tf.compat.v1.keras.layers.CuDNNGRU(units, \n",
        "                                    return_sequences=True, \n",
        "                                    return_state=True, \n",
        "                                    recurrent_initializer='glorot_uniform')\n",
        "    else:\n",
        "        print(\"NO: CuDNNGRU unavailable\")\n",
        "        return tf.keras.layers.GRU(units, \n",
        "                               return_sequences=True, \n",
        "                               return_state=True, \n",
        "                               recurrent_activation='relu', \n",
        "                               recurrent_initializer='glorot_uniform')\n",
        "\n",
        "### Build the model\n",
        "class EmoGRU(tf.keras.Model):\n",
        "    def __init__(self, vocab_inp_size, embedding_dim, hidden_units, batch_sz, output_size):\n",
        "        super(EmoGRU, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.hidden_units = hidden_units\n",
        "        \n",
        "        # layers\n",
        "        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_inp_size, output_dim=embedding_dim, input_shape=(inp_shape,))\n",
        "        self.gru = gru(self.hidden_units)\n",
        "        self.dropout = tf.keras.layers.Dropout(0.5)\n",
        "        self.fc = tf.keras.layers.Dense(output_size, activation=\"softmax\")\n",
        "        \n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x) \n",
        "        output, h_state = self.gru(x, initial_state = hidden) \n",
        "        out = output[:,-1,:]\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc(out) \n",
        "        return out, h_state\n",
        "    \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.hidden_units))\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "GRUmodel = EmoGRU(vocab_inp_size, embedding_dim, units, BATCH_SIZE, target_size)\n",
        "hidden = GRUmodel.initialize_hidden_state() # initialize the hidden state of the RNN"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-17-6e7c5c4768cd>:4: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n",
            "YES: CuDNNGRU available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0915cbc8-5a50-4477-c79d-f7bcf1a7f8aa",
        "id": "663W-eJnF0tL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# testing for the first batch only then break the for loop\n",
        "for (batch, (inp, targ)) in enumerate(train_dataset): \n",
        "    out, state = GRUmodel(inp, hidden)\n",
        "    break\n",
        "GRUmodel.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"emo_gru\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  16530432  \n",
            "_________________________________________________________________\n",
            "cu_dnngru (CuDNNGRU)         multiple                  3938304   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  5125      \n",
            "=================================================================\n",
            "Total params: 20,473,861\n",
            "Trainable params: 20,473,861\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNC7v_t7ZP3H",
        "colab_type": "text"
      },
      "source": [
        "### GRU training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSufHHUPzyVA",
        "colab_type": "code",
        "outputId": "ec9a0abc-7e77-4e81-e17a-e2f880807364",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "EPOCHS = 3\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    ### Initialize hidden state\n",
        "    hidden = GRUmodel.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "    train_accuracy, val_accuracy = 0, 0\n",
        "    \n",
        "    ### Training\n",
        "    for (batch, (inp, targ)) in enumerate(train_dataset): # add posts here \n",
        "        loss = 0\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions,_ = GRUmodel(inp, hidden)\n",
        "            emo_loss = Custom_KLD_loss(targ, predictions) #add posts here\n",
        "            loss += emo_loss\n",
        "            \n",
        "        batch_loss = (loss / int(targ.shape[1])) \n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        batch_accuracy = custom_accuracy(targ, predictions)\n",
        "        train_accuracy += batch_accuracy\n",
        "        \n",
        "        gradients = tape.gradient(loss, GRUmodel.trainable_variables)\n",
        "        optimizer.apply_gradients(grads_and_vars=(zip(gradients, GRUmodel.trainable_variables)))\n",
        "\n",
        "        if batch % 50 == 0:\n",
        "            print('Epoch {} Batch {} '.format(epoch + 1,batch))\n",
        "            \n",
        "    ### Validating\n",
        "    hidden = GRUmodel.initialize_hidden_state()\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(test_dataset):        \n",
        "        predictions,_ = GRUmodel(inp, hidden)        \n",
        "        batch_accuracy = custom_accuracy(targ, predictions)\n",
        "        val_accuracy += batch_accuracy\n",
        "    \n",
        "    print('Epoch {} -- Train Acc. {:.4f} -- Val Acc. {:.4f}'.format(epoch + 1, \n",
        "                                                             train_accuracy / TRAIN_N_BATCH,\n",
        "                                                             val_accuracy / TEST_N_BATCH))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 \n",
            "Epoch 1 Batch 50 \n",
            "Epoch 1 Batch 100 \n",
            "Epoch 1 Batch 150 \n",
            "Epoch 1 Batch 200 \n",
            "Epoch 1 Batch 250 \n",
            "Epoch 1 Batch 300 \n",
            "Epoch 1 Batch 350 \n",
            "Epoch 1 Batch 400 \n",
            "Epoch 1 -- Train Acc. 0.7844 -- Val Acc. 0.7353\n",
            "Time taken for 1 epoch 296.8420355319977 sec\n",
            "\n",
            "Epoch 2 Batch 0 \n",
            "Epoch 2 Batch 50 \n",
            "Epoch 2 Batch 100 \n",
            "Epoch 2 Batch 150 \n",
            "Epoch 2 Batch 200 \n",
            "Epoch 2 Batch 250 \n",
            "Epoch 2 Batch 300 \n",
            "Epoch 2 Batch 350 \n",
            "Epoch 2 Batch 400 \n",
            "Epoch 2 -- Train Acc. 0.8297 -- Val Acc. 0.7427\n",
            "Time taken for 1 epoch 296.3518843650818 sec\n",
            "\n",
            "Epoch 3 Batch 0 \n",
            "Epoch 3 Batch 50 \n",
            "Epoch 3 Batch 100 \n",
            "Epoch 3 Batch 150 \n",
            "Epoch 3 Batch 200 \n",
            "Epoch 3 Batch 250 \n",
            "Epoch 3 Batch 300 \n",
            "Epoch 3 Batch 350 \n",
            "Epoch 3 Batch 400 \n",
            "Epoch 3 -- Train Acc. 0.8676 -- Val Acc. 0.7432\n",
            "Time taken for 1 epoch 296.2948338985443 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDY9qziRXWh8",
        "colab_type": "text"
      },
      "source": [
        "### Constructing the LSTM model (as a subclass of keras Model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsBytMUwNFjw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "78998bfc-72a4-4b1c-b711-e95314fe7df2"
      },
      "source": [
        "### define the LSTM component\n",
        "def lstm(units):\n",
        "    if tf.test.is_gpu_available():\n",
        "        print(\"YES: CuDNNLSTM available\")\n",
        "        return tf.compat.v1.keras.layers.CuDNNLSTM(units, \n",
        "                                    return_sequences=True, \n",
        "                                    return_state=True, \n",
        "                                    recurrent_initializer='glorot_uniform')\n",
        "    else:\n",
        "        print(\"NO: CuDNNLSTM unavailable\")\n",
        "        return tf.keras.layers.LSTM(units, \n",
        "                               return_sequences=True, \n",
        "                               return_state=True, \n",
        "                               recurrent_activation='relu', \n",
        "                               recurrent_initializer='glorot_uniform')\n",
        "\n",
        "### Build the model\n",
        "class EmoLSTM(tf.keras.Model):\n",
        "    def __init__(self, vocab_inp_size, embedding_dim, hidden_units, batch_sz, output_size):\n",
        "        super(EmoLSTM, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.hidden_units = hidden_units\n",
        "        \n",
        "        # layers\n",
        "        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_inp_size, output_dim=embedding_dim, input_shape=(inp_shape,))\n",
        "        self.lstm = lstm(self.hidden_units)\n",
        "        self.dropout = tf.keras.layers.Dropout(0.5)\n",
        "        self.fc = tf.keras.layers.Dense(output_size, activation=\"softmax\")\n",
        "        \n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x) \n",
        "        output, h_state, state = self.lstm(x, initial_state = [hidden, hidden]) \n",
        "        out = output[:,-1,:]\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc(out) \n",
        "        return out, h_state\n",
        "    \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.hidden_units))\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "LSTMmodel = EmoLSTM(vocab_inp_size, embedding_dim, units, BATCH_SIZE, target_size)\n",
        "hidden = LSTMmodel.initialize_hidden_state() # initialize the hidden state of the RNN"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "YES: CuDNNLSTM available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjWjbw4RWuxN",
        "colab_type": "text"
      },
      "source": [
        "### LSTM training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX-PIIgcXPIB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "1a96e182-9238-42c9-eac3-df394ecd666c"
      },
      "source": [
        "opt = tf.keras.optimizers.Adam()\n",
        "\n",
        "EPOCHS = 4\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    total_loss = 0\n",
        "    train_accuracy, val_accuracy = 0, 0\n",
        "    \n",
        "    ### Training\n",
        "    for (batch, (inp, targ)) in enumerate(train_dataset):\n",
        "        loss = 0\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            # predictions = tf.convert_to_tensor(LSTMmodel.predict(inp, batch_size=BATCH_SIZE))\n",
        "            predictions,_ = LSTMmodel(inp, hidden)\n",
        "            emo_loss = Custom_KLD_loss(targ, predictions)\n",
        "            loss += emo_loss\n",
        "      \n",
        "        batch_loss = (loss / int(targ.shape[1])) \n",
        "        total_loss += batch_loss\n",
        "\n",
        "        batch_accuracy = custom_accuracy(targ, predictions)\n",
        "        train_accuracy += batch_accuracy\n",
        "        gradients = tape.gradient(loss, LSTMmodel.trainable_variables)\n",
        "        opt.apply_gradients(zip(gradients, LSTMmodel.trainable_variables))\n",
        "\n",
        "        if batch % 50 == 0:\n",
        "            print('Epoch {} Batch {} '.format(epoch + 1,batch))           \n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(test_dataset):        \n",
        "        predictions,_ = LSTMmodel(inp, hidden)        \n",
        "        batch_accuracy = custom_accuracy(targ, predictions)\n",
        "        val_accuracy += batch_accuracy\n",
        "    \n",
        "    print('Epoch {} -- Train Acc. {:.4f} -- Val Acc. {:.4f}'.format(epoch + 1, \n",
        "                                                             train_accuracy / TRAIN_N_BATCH,\n",
        "                                                             val_accuracy / TEST_N_BATCH))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 \n",
            "Epoch 1 Batch 50 \n",
            "Epoch 1 Batch 100 \n",
            "Epoch 1 Batch 150 \n",
            "Epoch 1 Batch 200 \n",
            "Epoch 1 Batch 250 \n",
            "Epoch 1 Batch 300 \n",
            "Epoch 1 Batch 350 \n",
            "Epoch 1 Batch 400 \n",
            "Epoch 1 -- Train Acc. 0.6210 -- Val Acc. 0.6062\n",
            "Time taken for 1 epoch 391.4581491947174 sec\n",
            "\n",
            "Epoch 2 Batch 0 \n",
            "Epoch 2 Batch 50 \n",
            "Epoch 2 Batch 100 \n",
            "Epoch 2 Batch 150 \n",
            "Epoch 2 Batch 200 \n",
            "Epoch 2 Batch 250 \n",
            "Epoch 2 Batch 300 \n",
            "Epoch 2 Batch 350 \n",
            "Epoch 2 Batch 400 \n",
            "Epoch 2 -- Train Acc. 0.6560 -- Val Acc. 0.6953\n",
            "Time taken for 1 epoch 396.314414024353 sec\n",
            "\n",
            "Epoch 3 Batch 0 \n",
            "Epoch 3 Batch 50 \n",
            "Epoch 3 Batch 100 \n",
            "Epoch 3 Batch 150 \n",
            "Epoch 3 Batch 200 \n",
            "Epoch 3 Batch 250 \n",
            "Epoch 3 Batch 300 \n",
            "Epoch 3 Batch 350 \n",
            "Epoch 3 Batch 400 \n",
            "Epoch 3 -- Train Acc. 0.7353 -- Val Acc. 0.7234\n",
            "Time taken for 1 epoch 401.0300145149231 sec\n",
            "\n",
            "Epoch 4 Batch 0 \n",
            "Epoch 4 Batch 50 \n",
            "Epoch 4 Batch 100 \n",
            "Epoch 4 Batch 150 \n",
            "Epoch 4 Batch 200 \n",
            "Epoch 4 Batch 250 \n",
            "Epoch 4 Batch 300 \n",
            "Epoch 4 Batch 350 \n",
            "Epoch 4 Batch 400 \n",
            "Epoch 4 -- Train Acc. 0.7910 -- Val Acc. 0.7355\n",
            "Time taken for 1 epoch 391.91170930862427 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKDUL1kMWeIy",
        "colab_type": "text"
      },
      "source": [
        "### CNN training (Sequential Keras Model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpiX6xNzWlAO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "4371d618-e98c-41fe-fbe9-724e6bfe8d28"
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(vocab_inp_size, 64, input_length=inp_shape))\n",
        "model.add(tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dropout(rate=0.4))\n",
        "model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(target_size, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.KLD,\n",
        "              metrics=[custom_accuracy])\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    epochs=4,\n",
        "                    batch_size=256,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    verbose=1)\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 69, 64)            4132608   \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 67, 64)            12352     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 4288)              0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 4288)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 50)                214450    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 5)                 255       \n",
            "=================================================================\n",
            "Total params: 4,359,665\n",
            "Trainable params: 4,359,665\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/4\n",
            "209/209 [==============================] - 10s 49ms/step - loss: 0.5851 - custom_accuracy: 0.6640 - val_loss: 0.4696 - val_custom_accuracy: 0.7232\n",
            "Epoch 2/4\n",
            "209/209 [==============================] - 10s 49ms/step - loss: 0.3908 - custom_accuracy: 0.7681 - val_loss: 0.4395 - val_custom_accuracy: 0.7424\n",
            "Epoch 3/4\n",
            "209/209 [==============================] - 10s 49ms/step - loss: 0.2892 - custom_accuracy: 0.8212 - val_loss: 0.4527 - val_custom_accuracy: 0.7434\n",
            "Epoch 4/4\n",
            "209/209 [==============================] - 10s 49ms/step - loss: 0.2269 - custom_accuracy: 0.8531 - val_loss: 0.4738 - val_custom_accuracy: 0.7401\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn-BBHpCzYCM",
        "colab_type": "text"
      },
      "source": [
        "# 5) Analysis of Polarity and Controversy from an emotion distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdis4FZMzfsJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b1375e09-5e12-464e-fe39-8a391573d285"
      },
      "source": [
        "from scipy.spatial import distance\n",
        "\n",
        "def compute_controversy(prediction):\n",
        "  uniform_distribution = [0.2, 0.2, 0.2, 0.2, 0.2]\n",
        "  return (1 - distance.jensenshannon(prediction, uniform_distribution, 2.0))\n",
        "  \n",
        "# examples\n",
        "res1 = compute_controversy([0.2, 0.2, 0.2, 0.2, 0.2])\n",
        "res2 = compute_controversy([1, 0, 0, 0, 0])\n",
        "print(\"Controversy from a uniform distribution: \", res1)\n",
        "print(\"Controversy from an imbalanced distribution:\", res2)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Controversy from a uniform distribution:  1.0\n",
            "Controversy from an imbalanced distribution: 0.21898364485051436\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22861, 69)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAWxgjqtz7hy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_polarity(prediction):\n",
        "  return ((prediction[0]+prediction[1]+prediction[2])/(prediction[0]+prediction[1]+prediction[2]+prediction[3]+prediction[4]))\n",
        "\n",
        "polarity_example = compute_polarity([0.2, 0.1, 0.1, 0.3, 0.3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmSlHPyHzKbA",
        "colab_type": "text"
      },
      "source": [
        "##--------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI6a55LWyyjH",
        "colab_type": "text"
      },
      "source": [
        "# 5) Engagement prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yh2wSdh79iL",
        "colab_type": "text"
      },
      "source": [
        "### Preparing the engagement datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4N1Wf5b789l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_eng_train, posts_train)).shuffle(TRAIN_BUFFER_SIZE) #add posts_training here for engagement prediction\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_eng_test, posts_test)).shuffle(TEST_BUFFER_SIZE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "output_size = y_eng_train.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUDDsmvD-C4G",
        "colab_type": "text"
      },
      "source": [
        "### Custom loss function for engagement training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceMVTyQA-Cfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_engagement_loss(y, yhat, posts_train):\n",
        "  binary_pred = tf.math.greater(yhat, 0.5)\n",
        "  batch_size = tf.cast(tf.shape(y)[0], dtype=tf.int32) \n",
        "  loss = tf.keras.losses.binary_crossentropy(y, yhat)\n",
        "  li = []\n",
        "\n",
        "  for i in range(batch_size):\n",
        "    loss_factor = 1\n",
        "    if (posts_train[i]==\"video\" or posts_train[i]==\"photo\"):\n",
        "      ground_sample = tf.cast(y[i], tf.int32)\n",
        "      pred_sample = tf.cast(binary_pred[i], tf.int32)\n",
        "      \n",
        "      if (ground_sample[2]==1 and pred_sample[0]==1):\n",
        "        loss_factor = loss_factor+1\n",
        "      if (ground_sample[5]==1 and pred_sample[3]==1):\n",
        "        loss_factor = loss_factor+1\n",
        "      if (ground_sample[8]==1 and pred_sample[6]==1):\n",
        "        loss_factor = loss_factor+1\n",
        "    \n",
        "    li.append(loss_factor)\n",
        "\n",
        "  ma = tf.convert_to_tensor(li)\n",
        "  mask = tf.cast(ma, dtype=tf.float32)\n",
        "  return loss*mask\n",
        "\n",
        "def eng_accuracy(y, yhat):\n",
        "  bin_yhat = tf.math.greater(yhat, 0.5)\n",
        "  match = tf.math.equal(tf.cast(y, tf.int64), tf.cast(bin_yhat, tf.int64))\n",
        "  n_matches = tf.reduce_sum(tf.cast(match, tf.float32))\n",
        "  len_y = tf.cast(tf.size(y), tf.float32)\n",
        "  acc = (n_matches/(len_y))\n",
        "  return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_FjfJCn7DHk",
        "colab_type": "text"
      },
      "source": [
        "### Constructing the engagement GRU model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M38KGGQu7Bhc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "482fef09-9b75-45b1-d0c4-4b84625462bb"
      },
      "source": [
        "### define the GRU component\n",
        "def gru(units):\n",
        "  # Using CuDNNGRU(provides a 3x speedup than GRU) when it is available\n",
        "  # the code automatically does that.\n",
        "    if tf.test.is_gpu_available():\n",
        "        print(\"YES: CuDNNGRU available\")\n",
        "        return tf.compat.v1.keras.layers.CuDNNGRU(units, \n",
        "                                    return_sequences=True, \n",
        "                                    return_state=True, \n",
        "                                    recurrent_initializer='glorot_uniform')\n",
        "    else:\n",
        "        print(\"NO: CuDNNGRU unavailable\")\n",
        "        return tf.keras.layers.GRU(units, \n",
        "                               return_sequences=True, \n",
        "                               return_state=True, \n",
        "                               recurrent_activation='relu', \n",
        "                               recurrent_initializer='glorot_uniform')\n",
        "\n",
        "### Build the model\n",
        "class EmoGRU(tf.keras.Model):\n",
        "    def __init__(self, vocab_inp_size, embedding_dim, hidden_units, batch_sz, output_size):\n",
        "        super(EmoGRU, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.hidden_units = hidden_units\n",
        "        \n",
        "        # layers\n",
        "        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_inp_size, output_dim=embedding_dim, input_shape=(inp_shape,))\n",
        "        self.gru = gru(self.hidden_units)\n",
        "        self.dropout = tf.keras.layers.Dropout(0.5)\n",
        "        self.fc = tf.keras.layers.Dense(output_size, activation=\"sigmoid\")\n",
        "        \n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x) \n",
        "        output, h_state = self.gru(x, initial_state = hidden) \n",
        "        out = output[:,-1,:]\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc(out) \n",
        "        return out, h_state\n",
        "    \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.hidden_units))\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "GRUmodel = EmoGRU(vocab_inp_size, embedding_dim, units, BATCH_SIZE, output_size)\n",
        "hidden = GRUmodel.initialize_hidden_state() # initialize the hidden state of the RNN"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "YES: CuDNNGRU available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yv_Jp_693RW",
        "colab_type": "text"
      },
      "source": [
        "### GRU training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvlieJzZ9z9-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "c6152670-c5c6-42b9-8c92-a463ac290da6"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "EPOCHS = 4\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    ### Initialize hidden state\n",
        "    hidden = GRUmodel.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "    train_accuracy, val_accuracy = 0, 0\n",
        "    \n",
        "    ### Training\n",
        "    for (batch, (inp, targ, post_type)) in enumerate(train_dataset): # add posts here \n",
        "        loss = 0\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions,_ = GRUmodel(inp, hidden)\n",
        "            engagement_loss = custom_engagement_loss(targ, predictions, post_type) #add posts here\n",
        "            loss += engagement_loss\n",
        "            \n",
        "        batch_loss = (loss / int(targ.shape[1])) \n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        batch_accuracy = eng_accuracy(targ, predictions)\n",
        "        train_accuracy += batch_accuracy\n",
        "        \n",
        "        gradients = tape.gradient(loss, GRUmodel.trainable_variables)\n",
        "        optimizer.apply_gradients(grads_and_vars=(zip(gradients, GRUmodel.trainable_variables)))\n",
        "\n",
        "        if batch % 50 == 0:\n",
        "            print('Epoch {} Batch {} '.format(epoch + 1,batch))\n",
        "            \n",
        "    ### Validating\n",
        "    hidden = GRUmodel.initialize_hidden_state()\n",
        "\n",
        "    for (batch, (inp, targ, post_type)) in enumerate(test_dataset):        \n",
        "        predictions,_ = GRUmodel(inp, hidden)        \n",
        "        batch_accuracy = eng_accuracy(targ, predictions)\n",
        "        val_accuracy += batch_accuracy\n",
        "    \n",
        "    print('Epoch {} -- Train Acc. {:.4f} -- Val Acc. {:.4f}'.format(epoch + 1, \n",
        "                                                             train_accuracy / TRAIN_N_BATCH,\n",
        "                                                             val_accuracy / TEST_N_BATCH))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 \n",
            "Epoch 1 Batch 50 \n",
            "Epoch 1 Batch 100 \n",
            "Epoch 1 Batch 150 \n",
            "Epoch 1 Batch 200 \n",
            "Epoch 1 Batch 250 \n",
            "Epoch 1 Batch 300 \n",
            "Epoch 1 Batch 350 \n",
            "Epoch 1 Batch 400 \n",
            "Epoch 1 -- Train Acc. 0.6702 -- Val Acc. 0.6837\n",
            "Time taken for 1 epoch 146.8789746761322 sec\n",
            "\n",
            "Epoch 2 Batch 0 \n",
            "Epoch 2 Batch 50 \n",
            "Epoch 2 Batch 100 \n",
            "Epoch 2 Batch 150 \n",
            "Epoch 2 Batch 200 \n",
            "Epoch 2 Batch 250 \n",
            "Epoch 2 Batch 300 \n",
            "Epoch 2 Batch 350 \n",
            "Epoch 2 Batch 400 \n",
            "Epoch 2 -- Train Acc. 0.7028 -- Val Acc. 0.6942\n",
            "Time taken for 1 epoch 146.95168161392212 sec\n",
            "\n",
            "Epoch 3 Batch 0 \n",
            "Epoch 3 Batch 50 \n",
            "Epoch 3 Batch 100 \n",
            "Epoch 3 Batch 150 \n",
            "Epoch 3 Batch 200 \n",
            "Epoch 3 Batch 250 \n",
            "Epoch 3 Batch 300 \n",
            "Epoch 3 Batch 350 \n",
            "Epoch 3 Batch 400 \n",
            "Epoch 3 -- Train Acc. 0.7452 -- Val Acc. 0.6848\n",
            "Time taken for 1 epoch 146.21924495697021 sec\n",
            "\n",
            "Epoch 4 Batch 0 \n",
            "Epoch 4 Batch 50 \n",
            "Epoch 4 Batch 100 \n",
            "Epoch 4 Batch 150 \n",
            "Epoch 4 Batch 200 \n",
            "Epoch 4 Batch 250 \n",
            "Epoch 4 Batch 300 \n",
            "Epoch 4 Batch 350 \n",
            "Epoch 4 Batch 400 \n",
            "Epoch 4 -- Train Acc. 0.7896 -- Val Acc. 0.6650\n",
            "Time taken for 1 epoch 146.19158363342285 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DgMnmm1Xkwp",
        "colab_type": "text"
      },
      "source": [
        "### Constructing LSTM engagement model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECscVWYkXr-p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7d1866ba-8e5b-4d8f-c280-b99181031c31"
      },
      "source": [
        "### define the LSTM component\n",
        "def lstm(units):\n",
        "    if tf.test.is_gpu_available():\n",
        "        print(\"YES: CuDNNLSTM available\")\n",
        "        return tf.compat.v1.keras.layers.CuDNNLSTM(units, \n",
        "                                    return_sequences=True, \n",
        "                                    return_state=True, \n",
        "                                    recurrent_initializer='glorot_uniform')\n",
        "    else:\n",
        "        print(\"NO: CuDNNLSTM unavailable\")\n",
        "        return tf.keras.layers.LSTM(units, \n",
        "                               return_sequences=True, \n",
        "                               return_state=True, \n",
        "                               recurrent_activation='relu', \n",
        "                               recurrent_initializer='glorot_uniform')\n",
        "\n",
        "### Build the model\n",
        "class EmoLSTM(tf.keras.Model):\n",
        "    def __init__(self, vocab_inp_size, embedding_dim, hidden_units, batch_sz, output_size):\n",
        "        super(EmoLSTM, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.hidden_units = hidden_units\n",
        "        \n",
        "        # layers\n",
        "        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_inp_size, output_dim=embedding_dim, input_shape=(inp_shape,))\n",
        "        self.lstm = lstm(self.hidden_units)\n",
        "        self.dropout = tf.keras.layers.Dropout(0.5)\n",
        "        self.fc = tf.keras.layers.Dense(output_size, activation=\"sigmoid\")\n",
        "        \n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x) \n",
        "        output, h_state, state = self.lstm(x, initial_state = [hidden, hidden]) \n",
        "        out = output[:,-1,:]\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc(out) \n",
        "        return out, h_state\n",
        "    \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.hidden_units))\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "LSTMmodel = EmoLSTM(vocab_inp_size, embedding_dim, units, BATCH_SIZE, output_size)\n",
        "hidden = LSTMmodel.initialize_hidden_state() # initialize the hidden state of the RNN"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "YES: CuDNNLSTM available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G19PIOUcX3XL",
        "colab_type": "text"
      },
      "source": [
        "### LSTM training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-4-RFO0YJ_G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "681a74fd-13d7-4bb4-9f2e-be8fdfd77111"
      },
      "source": [
        "opt = tf.keras.optimizers.Adam()\n",
        "\n",
        "EPOCHS = 4\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    total_loss = 0\n",
        "    train_accuracy, val_accuracy = 0, 0\n",
        "    \n",
        "    ### Training\n",
        "    for (batch, (inp, targ, posts_type)) in enumerate(train_dataset):\n",
        "        loss = 0\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            # predictions = tf.convert_to_tensor(LSTMmodel.predict(inp, batch_size=BATCH_SIZE))\n",
        "            predictions,_ = LSTMmodel(inp, hidden)\n",
        "            emo_loss = custom_engagement_loss(targ, predictions, posts_type)\n",
        "            loss += emo_loss\n",
        "      \n",
        "        batch_loss = (loss / int(targ.shape[1])) \n",
        "        total_loss += batch_loss\n",
        "\n",
        "        batch_accuracy = eng_accuracy(targ, predictions)\n",
        "        train_accuracy += batch_accuracy\n",
        "        gradients = tape.gradient(loss, LSTMmodel.trainable_variables)\n",
        "        opt.apply_gradients(zip(gradients, LSTMmodel.trainable_variables))\n",
        "\n",
        "        if batch % 50 == 0:\n",
        "            print('Epoch {} Batch {} '.format(epoch + 1,batch))           \n",
        "\n",
        "    for (batch, (inp, targ, posts_type)) in enumerate(test_dataset):        \n",
        "        predictions,_ = LSTMmodel(inp, hidden)        \n",
        "        batch_accuracy = eng_accuracy(targ, predictions)\n",
        "        val_accuracy += batch_accuracy\n",
        "    \n",
        "    print('Epoch {} -- Train Acc. {:.4f} -- Val Acc. {:.4f}'.format(epoch + 1, \n",
        "                                                             train_accuracy / TRAIN_N_BATCH,\n",
        "                                                             val_accuracy / TEST_N_BATCH))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 \n",
            "Epoch 1 Batch 50 \n",
            "Epoch 1 Batch 100 \n",
            "Epoch 1 Batch 150 \n",
            "Epoch 1 Batch 200 \n",
            "Epoch 1 Batch 250 \n",
            "Epoch 1 Batch 300 \n",
            "Epoch 1 Batch 350 \n",
            "Epoch 1 Batch 400 \n",
            "Epoch 1 -- Train Acc. 0.6666 -- Val Acc. 0.6667\n",
            "Time taken for 1 epoch 185.63288617134094 sec\n",
            "\n",
            "Epoch 2 Batch 0 \n",
            "Epoch 2 Batch 50 \n",
            "Epoch 2 Batch 100 \n",
            "Epoch 2 Batch 150 \n",
            "Epoch 2 Batch 200 \n",
            "Epoch 2 Batch 250 \n",
            "Epoch 2 Batch 300 \n",
            "Epoch 2 Batch 350 \n",
            "Epoch 2 Batch 400 \n",
            "Epoch 2 -- Train Acc. 0.6642 -- Val Acc. 0.6667\n",
            "Time taken for 1 epoch 188.60984063148499 sec\n",
            "\n",
            "Epoch 3 Batch 0 \n",
            "Epoch 3 Batch 50 \n",
            "Epoch 3 Batch 100 \n",
            "Epoch 3 Batch 150 \n",
            "Epoch 3 Batch 200 \n",
            "Epoch 3 Batch 250 \n",
            "Epoch 3 Batch 300 \n",
            "Epoch 3 Batch 350 \n",
            "Epoch 3 Batch 400 \n",
            "Epoch 3 -- Train Acc. 0.6678 -- Val Acc. 0.6713\n",
            "Time taken for 1 epoch 187.5673303604126 sec\n",
            "\n",
            "Epoch 4 Batch 0 \n",
            "Epoch 4 Batch 50 \n",
            "Epoch 4 Batch 100 \n",
            "Epoch 4 Batch 150 \n",
            "Epoch 4 Batch 200 \n",
            "Epoch 4 Batch 250 \n",
            "Epoch 4 Batch 300 \n",
            "Epoch 4 Batch 350 \n",
            "Epoch 4 Batch 400 \n",
            "Epoch 4 -- Train Acc. 0.6906 -- Val Acc. 0.6822\n",
            "Time taken for 1 epoch 185.68398427963257 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3i1p0hBXdDu",
        "colab_type": "text"
      },
      "source": [
        "### CNN engagement model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ONvtZBZ0METI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "a28f5f35-98f1-4948-e283-99aab0e6f573"
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(vocab_inp_size, 64, input_length=inp_shape))\n",
        "model.add(tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dropout(rate=0.4))\n",
        "model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(output_size, activation='sigmoid'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.binary_crossentropy,\n",
        "              metrics=[eng_accuracy])\n",
        "history = model.fit(x_train,\n",
        "                    y_eng_train,\n",
        "                    epochs=4,\n",
        "                    batch_size=256,\n",
        "                    validation_data=(x_test, y_eng_test),\n",
        "                    verbose=1)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 69, 64)            4132608   \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 67, 64)            12352     \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 4288)              0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 4288)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 50)                214450    \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 9)                 459       \n",
            "=================================================================\n",
            "Total params: 4,359,869\n",
            "Trainable params: 4,359,869\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/4\n",
            "209/209 [==============================] - 9s 43ms/step - loss: 0.6123 - eng_accuracy: 0.6750 - val_loss: 0.5881 - val_eng_accuracy: 0.6894\n",
            "Epoch 2/4\n",
            "209/209 [==============================] - 9s 42ms/step - loss: 0.5541 - eng_accuracy: 0.7130 - val_loss: 0.5800 - val_eng_accuracy: 0.6956\n",
            "Epoch 3/4\n",
            "209/209 [==============================] - 9s 42ms/step - loss: 0.4871 - eng_accuracy: 0.7547 - val_loss: 0.6028 - val_eng_accuracy: 0.6821\n",
            "Epoch 4/4\n",
            "209/209 [==============================] - 9s 42ms/step - loss: 0.4030 - eng_accuracy: 0.8086 - val_loss: 0.6511 - val_eng_accuracy: 0.6720\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
